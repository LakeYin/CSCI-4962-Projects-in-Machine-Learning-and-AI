{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.6rc1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6rc1 64-bit"
    },
    "interpreter": {
      "hash": "965a33b357934e9f83fc7ef133db771ae96b140076b3a892fdf40d2537a7764b"
    },
    "colab": {
      "name": "hw4.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhxD0Zt-x-iy"
      },
      "source": [
        "# Import dataset\n",
        "We will be trying to classify IMDB reviews as either positive or negative. Sequence models are useful for this task since text is just a sequence of words and a sequence model can learn the pattern of these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP9G8bxdx-jL"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "train_set = tfds.load('imdb_reviews', split='train', as_supervised=True).take(5000)\n",
        "test_set = tfds.load('imdb_reviews', split='test', as_supervised=True).take(1000)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E3_8hI5x-jZ"
      },
      "source": [
        "# Prepare tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1uhgqSIx-jd"
      },
      "source": [
        "max_text_length = 0\n",
        "\n",
        "# training\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i, j in train_set:\n",
        "    i = str(i.numpy())\n",
        "    max_text_length = max(max_text_length, len(i))\n",
        "    X_train.append(i)\n",
        "    y_train.append(int(j))\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_text_length, padding='post')\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# testing\n",
        "X_test = []\n",
        "y_test = []\n",
        "for i, j in test_set:\n",
        "    i = str(i.numpy())\n",
        "    X_test.append(i)\n",
        "    y_test.append(int(j))\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_text_length, padding='post')\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbl8Mr0Rx-jh"
      },
      "source": [
        "# Build the RNN\n",
        "An embedding layer is needed to convert sequences of tokens into sequences of vectors that can be easily understood by the RNN layer, which uses tanh activation to take advantage of GPU optimizations. This is then fed into a layer for binary classification with sigmoid activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z68kY8uyx-jk"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(10001, 64, mask_zero=True))\n",
        "model.add(keras.layers.SimpleRNN(128))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy()])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL8aNwG0x-jp"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvAo3Yk3x-jt",
        "outputId": "82a48d64-9c44-4000-aafa-0e0182955eab"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "model.fit(X_train, y_train, epochs=epochs, validation_split=0.1, batch_size=256)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "18/18 [==============================] - 835s 46s/step - loss: 0.6982 - binary_accuracy: 0.5151 - val_loss: 0.6914 - val_binary_accuracy: 0.5360\n",
            "Epoch 2/5\n",
            "18/18 [==============================] - 828s 46s/step - loss: 0.6476 - binary_accuracy: 0.6631 - val_loss: 0.7092 - val_binary_accuracy: 0.4960\n",
            "Epoch 3/5\n",
            "18/18 [==============================] - 839s 47s/step - loss: 0.6216 - binary_accuracy: 0.6524 - val_loss: 0.7315 - val_binary_accuracy: 0.4820\n",
            "Epoch 4/5\n",
            "18/18 [==============================] - 868s 48s/step - loss: 0.5462 - binary_accuracy: 0.7520 - val_loss: 0.7160 - val_binary_accuracy: 0.5400\n",
            "Epoch 5/5\n",
            "18/18 [==============================] - 864s 48s/step - loss: 0.4547 - binary_accuracy: 0.8260 - val_loss: 0.6810 - val_binary_accuracy: 0.6000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb2de446d50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66PAK1msASIf"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImLBXPqFAidz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4a4336-86ab-4538-af1c-9b0eb1b37fa4"
      },
      "source": [
        "print(model.metrics_names)\n",
        "print(model.evaluate(X_test, y_test, verbose=0))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'binary_accuracy']\n",
            "[0.6651999950408936, 0.6190000176429749]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ouf3UmOyusq"
      },
      "source": [
        "# Create new model with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5xc-zpRy2XT"
      },
      "source": [
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Embedding(10001, 64, mask_zero=True))\n",
        "model2.add(keras.layers.LSTM(128))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy()])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9WLW8tzGWQ"
      },
      "source": [
        "# Train the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moIqHpAqzJrn",
        "outputId": "89c1283b-d58d-4b8b-9b52-e579eb813dbe"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "model2.fit(X_train, y_train, epochs=epochs, validation_split=0.1, batch_size=64) # batch size reduced due to colab ram limits"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "71/71 [==============================] - 21s 222ms/step - loss: 0.6879 - binary_accuracy: 0.5638 - val_loss: 0.6598 - val_binary_accuracy: 0.6100\n",
            "Epoch 2/5\n",
            "71/71 [==============================] - 14s 201ms/step - loss: 0.5184 - binary_accuracy: 0.7518 - val_loss: 0.4203 - val_binary_accuracy: 0.8200\n",
            "Epoch 3/5\n",
            "71/71 [==============================] - 14s 201ms/step - loss: 0.2508 - binary_accuracy: 0.9062 - val_loss: 0.4195 - val_binary_accuracy: 0.8300\n",
            "Epoch 4/5\n",
            "71/71 [==============================] - 14s 200ms/step - loss: 0.1470 - binary_accuracy: 0.9464 - val_loss: 0.4420 - val_binary_accuracy: 0.8200\n",
            "Epoch 5/5\n",
            "71/71 [==============================] - 14s 202ms/step - loss: 0.0907 - binary_accuracy: 0.9689 - val_loss: 0.4965 - val_binary_accuracy: 0.8200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb30e14e0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6XKabSqzbfh"
      },
      "source": [
        "# Evaluate LSTM on test set\n",
        "The LSTM is obviously much better than the SimpleRNN when predicting the sentiment of IMDB reviews. This is because the LSTM can remember longer patterns in the sentences. In comparison, the SimpleRNN can only remember one previous word for each cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF-tj1usze8v",
        "outputId": "38b1478f-ece4-4883-f334-8712760cbfdd"
      },
      "source": [
        "print(model2.metrics_names)\n",
        "print(model2.evaluate(X_test, y_test, verbose=0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'binary_accuracy']\n",
            "[0.5765174627304077, 0.7770000100135803]\n"
          ]
        }
      ]
    }
  ]
}