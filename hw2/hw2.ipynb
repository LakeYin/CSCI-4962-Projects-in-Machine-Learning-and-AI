{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create models for DecisionTreeClassifier\r\n",
    "\r\n",
    "When the max depth of the tree is restricted to 1, the accuracy decreases to around 66%. This is because at max_depth=1, the tree can only have 2 branches and therefore is only able to predict 2 out of the 3 classes in the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "iris = load_iris()\r\n",
    "X, y = shuffle(iris.data, iris.target)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\r\n",
    "depth_model = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\r\n",
    "\r\n",
    "print(f\"Standard: {accuracy_score(y_test, model.predict(X_test))}\")\r\n",
    "print(f\"Limited depth: {accuracy_score(y_test, depth_model.predict(X_test))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Standard: 0.9555555555555556\n",
      "Limited depth: 0.6666666666666666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create bagged and boosted models where max_depth=1, then cross-validate to measure performance\r\n",
    "Although both models perform well when max_depth=1, the boosted method performs better than the bagged method. This is likely because since each tree can only seperate 2 classes, the boost classifer makes the other trees focus on the classes the previous tree could not detect, thus maintaining coverage of all 3 classes.\r\n",
    "\r\n",
    "In comparison, the bagged model would have to depend on the random distribution of each subset to ensure that some trees cover classes not covered by others. Often, none of the the bagged trees will correctly identify the third class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "bag_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=5).fit(X, y)\r\n",
    "boost_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=5).fit(X, y)\r\n",
    "\r\n",
    "print(f\"Bagged: {cross_val_score(bag_model, X_train, y_train).mean()}\")\r\n",
    "print(f\"Boosted: {cross_val_score(boost_model, X_train, y_train).mean()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bagged: 0.7714285714285715\n",
      "Boosted: 0.9333333333333333\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare all 4 models\r\n",
    "\r\n",
    "I chose to use precision because given 3 classes, it is easy to see which classes perform poorly and how that poor performance affects the other classification scores due to how decision trees work. For example, the precision score for the max_depth=1 tree makes it obvious that 2 classes were being lumped into 1 group, and you can tell how they were lumped together because of how precision accounts for false positives.\r\n",
    "\r\n",
    "Using a different metric, such as an accuracy score, would not be able to capture the nuances of 3 classes. Previously in this notebook, accuracy scores were used which did not make it clear how each class was being predicted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "print(\"Standard Decision Tree:\")\r\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))\r\n",
    "print(f\"Precision: {precision_score(y_test, model.predict(X_test), average=None)}\")\r\n",
    "\r\n",
    "print(\"Small Decision Tree:\")\r\n",
    "print(confusion_matrix(y_test, depth_model.predict(X_test)))\r\n",
    "print(f\"Precision: {precision_score(y_test, depth_model.predict(X_test), average=None)}\")\r\n",
    "\r\n",
    "print(\"Bagged Decision Trees:\")\r\n",
    "print(confusion_matrix(y_test, bag_model.predict(X_test)))\r\n",
    "print(f\"Precision: {precision_score(y_test, bag_model.predict(X_test), average=None)}\")\r\n",
    "\r\n",
    "print(\"Boosted Decision Trees:\")\r\n",
    "print(confusion_matrix(y_test, boost_model.predict(X_test)))\r\n",
    "print(f\"Precision: {precision_score(y_test, boost_model.predict(X_test), average=None)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Standard Decision Tree:\n",
      "[[16  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  1 14]]\n",
      "Precision: [1.         0.92857143 0.93333333]\n",
      "Small Decision Tree:\n",
      "[[16  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0 15  0]]\n",
      "Precision: [1.         0.48275862 0.        ]\n",
      "Bagged Decision Trees:\n",
      "[[16  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0 15]]\n",
      "Precision: [1. 1. 1.]\n",
      "Boosted Decision Trees:\n",
      "[[16  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0 15]]\n",
      "Precision: [1. 1. 1.]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\yinl3\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6rc1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6rc1 64-bit"
  },
  "interpreter": {
   "hash": "965a33b357934e9f83fc7ef133db771ae96b140076b3a892fdf40d2537a7764b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}